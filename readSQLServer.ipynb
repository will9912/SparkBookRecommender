{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leer datos de SQL Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports y creación de sesión Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crea una SparkSession\n",
    "spark = SparkSession.builder.appName(\"Read SQL Server Data\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión con Base SQL Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de conexión\n",
    "host = \"library-usco.database.windows.net\"\n",
    "port = \"1433\"\n",
    "user = \"admin_library@library-usco\"\n",
    "password = \"Seminariousco123\"\n",
    "db = \"library\"\n",
    "driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del JDBC de SQL Server\n",
    "url_conn = f\"jdbc:sqlserver://{host}:{port};database={db};user={user};password={password};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.database.windows.net;loginTimeout=30;\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query de ingesta de datos y transformación base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query de consulta a SQL Server\n",
    "sql = \"\"\"\n",
    "SELECT * \n",
    "FROM mi_tabla \n",
    "WHERE mi_condicion\n",
    "\"\"\"\n",
    "\n",
    "# Leer datos de la base de datos\n",
    "df = spark.read.jdbc(url=url_conn, table=\"(\"+sql+\") as t\")\n",
    "df.createOrReplaceTempView(\"TABLE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformación de datos\n",
    "sql_code = \"\"\"\n",
    "-- INSERTAR CÓDIGO DE TRANSFORMACIÓN AQUÍ\n",
    "\"\"\"\n",
    "df_transformed = spark.sql(sql_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escribir los datos transformados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribe los datos en formato CSV\n",
    "csv_name = \"data/datos_base.csv\"\n",
    "df_transformed.write.csv(csv_name)\n",
    "\n",
    "\n",
    "print(f\"Los datos de Oracle se leyeron correctamente y se guardaron como {csv_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
