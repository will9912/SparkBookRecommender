{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e3186ba-2afb-429c-b026-d2130b45e63d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb9e5bc7-a0c6-470d-9467-3b2a66d43bf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga de libros y ratings--\n"
     ]
    }
   ],
   "source": [
    "# Carga de datos\n",
    "ratings = (spark.read\n",
    "  .format(\"sqlserver\")\n",
    "  .option(\"host\", \"library-usco.database.windows.net\")\n",
    "  .option(\"port\", \"1433\") # optional, can use default port 1433 if omitted\n",
    "  .option(\"user\", \"admin_library@library-usco\")\n",
    "  .option(\"password\", \"Seminariousco123\")\n",
    "  .option(\"database\", \"library\")\n",
    "  .option(\"dbtable\", \"dbo.Rating\") # (if schemaName not provided, default to \"dbo\")\n",
    "  .load()\n",
    ")\n",
    "ratings.createOrReplaceTempView('ratings')\n",
    "\n",
    "ratings = spark.sql(''' SELECT UserId, ISBN, BookRating FROM ratings ''')\n",
    "\n",
    "books = (spark.read\n",
    "  .format(\"sqlserver\")\n",
    "  .option(\"host\", \"library-usco.database.windows.net\")\n",
    "  .option(\"port\", \"1433\") # optional, can use default port 1433 if omitted\n",
    "  .option(\"user\", \"admin_library@library-usco\")\n",
    "  .option(\"password\", \"Seminariousco123\")\n",
    "  .option(\"database\", \"library\")\n",
    "  .option(\"dbtable\", \"dbo.Book\") # (if schemaName not provided, default to \"dbo\")\n",
    "  .load()\n",
    ")\n",
    "books.createOrReplaceTempView('books')\n",
    "\n",
    "books = spark.sql(''' SELECT ISBN, BookTitle, BookAuthor, YearOfPublication, Publisher FROM books ''')\n",
    "print(\"Carga de libros y ratings--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8746ef5f-ec17-4133-9eb2-b3f63bd296a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# convert string to int for ALS\n",
    "stringToInt = StringIndexer(inputCol='ISBN', outputCol='ISBN_int').fit(ratings)\n",
    "ratings = stringToInt.transform(ratings)\n",
    "\n",
    "# split data into training and test datatset\n",
    "train_df, test_df = ratings.randomSplit([0.8,0.2])\n",
    "\n",
    "# ALS model\n",
    "rec_model = ALS( maxIter=10 ,regParam=0.01,userCol='UserId',itemCol='ISBN_int',ratingCol='BookRating', \n",
    "                nonnegative=True, coldStartStrategy=\"drop\")\n",
    "\n",
    "rec_model = rec_model.fit(train_df)\n",
    "\n",
    "# making predictions on test set \n",
    "predicted_ratings=rec_model.transform(test_df)\n",
    "\n",
    "# calculate RMSE\n",
    "evaluator = RegressionEvaluator(metricName='rmse', predictionCol='prediction',labelCol='BookRating')\n",
    "rmse = evaluator.evaluate(predicted_ratings)\n",
    "\n",
    "# function to recommend top-n books for a user using trained model\n",
    "def recommend_for_user(user_id, n):\n",
    "    ratings_user = ratings.filter(col('UserId')==user_id)\n",
    "    pred_ratings_user = rec_model.transform(ratings_user.filter(col('BookRating')!=0))\n",
    "    recs_user = books.join(pred_ratings_user.select(['ISBN', 'prediction']), on='ISBN')\n",
    "    recs_user = recs_user.sort('prediction', ascending=False).drop('prediction').limit(n)\n",
    "    return recs_user\n",
    "\n",
    "# Obtener la lista de usuarios distintos\n",
    "user_list = ratings.select(\"UserId\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Generar 5 recomendaciones para cada usuario\n",
    "recommendations = []\n",
    "for user_id in user_list:\n",
    "    recs_user = recommend_for_user(user_id, 5)\n",
    "    recommendations.append(recs_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17218274-d0c0-4606-8c4d-0982be17165c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1eb99d-3f70-478a-9893-7b355591ca0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from datetime import datetime\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for i, recs_user in enumerate(recommendations):\n",
    "    df_con_nueva_columna = recs_user.withColumn(\"UserId\", lit(user_list[i]))\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    df_con_nueva_columna = df_con_nueva_columna.withColumn(\"date\", lit(date))\n",
    "    dfs.append(df_con_nueva_columna.drop(\"BookTitle\", \"BookAuthor\", \"YearOfPublication\", \"Publisher\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "751b6cd3-53b6-496d-90db-63e4bcb23b56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_complete = reduce(DataFrame.unionAll, dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b770d922-a8ff-4491-8159-9147fd08452e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_complete.write \\\n",
    "  .format(\"sqlserver\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .option(\"host\", \"library-usco.database.windows.net\") \\\n",
    "  .option(\"port\", \"1433\") \\\n",
    "  .option(\"user\", \"admin_library@library-usco\") \\\n",
    "  .option(\"password\", \"Seminariousco123\") \\\n",
    "  .option(\"database\", \"library\") \\\n",
    "  .option(\"dbtable\", \"dbo.User_Recomendation\") \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1759247951972040,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "library_notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
